\documentclass[a4paper,10pt,twocolumn]{article} %{{{1
%\documentclass[a4paper,10pt]{article} 
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}

\usepackage{epsfig} %for Ã¥ lime inn filer

\usepackage{algorithmic}
\usepackage{algorithm}  
%\usepackage{babel}  

\newcommand{\ts}[1]{\textbf{#1}}
\newcommand{\bra}[1]{\langle{#1}|}
\newcommand{\ket}[1]{|#1\rangle{}}
%\newcommand{\braket}[1][2]{\langle{#1}|#2\rangle{}}
\newcommand{\abs}[1]{\left|{#1}\right|}
\newcommand{\expec}[1]{\langle{}{#1}\rangle{}}

%Define theorem enviroment. 
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{assumption}[theorem]{Assumption}


\newenvironment{proof}[1][Proof]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{definition}[1][Definition]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{example}[1][Example]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newenvironment{remark}[1][Remark]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
\newcommand{\qed}{\nobreak \ifvmode \relax \else
      \ifdim\lastskip<1.5em \hskip-\lastskip
      \hskip1.5em plus0em minus0.5em \fi \nobreak
      \vrule height0.75em width0.5em depth0.25em\fi} %}}}1

\title{ Project 1, Fys 4411 \\ }
\author{Karl R. Leikanger}

\begin{document}

\maketitle
\tableofcontents

\begin{abstract}%{{{1
blablabla
\end{abstract}%}}}1

\section{Quantum dots}% {{{1

Our system of choice 

%}}}1

\section{The Monte Carlo method}%{{{1

\subsection{Introduction}%{{{2
Monte Carlo (MC) methods includes a wide range of computational methods for solving integrals. 
The main idea is to sample the integrand $f(\vec r)$ in random points $\{\vec r_i\}$, 
and finding an approximate solution to the integral by doing statistical analysis on the sampling data. 
The statistical error will give us an immediate idea of the precicion of the result.
%Statistical measures will give us an idea of the quality of the results

In general, any integral can be written on the form
\begin{align}
\int_{\mathcal D} f(\vec r) d\vec r = 
\int_{\mathcal D} P(\vec r) g(\vec r) d\vec r,
\end{align} 
where $P(\vec r)$ is a probability distribution. The integral can now be approximated by the sum 
\begin{align}
	\int_{\mathcal D} f(\vec r) d\vec r 
\approx \frac1N\sum_{i=1}^Ng(\vec r_i),
\end{align} 
where $\{\vec{r_i}\}$ is a set of random numbers drawn from the statistical distribution $P(\vec r)$.

The simplest example, possible whenever the region of the of the integral is finite,
is when $P$ is a uniform distribution bounded by the integration limits. Then
$P(\vec r)$ is a constant and $g(\vec r)=f(\ts r)$. However, for many integrals this method 
will converge slowly and will require a very large number of samples to
give accurate results. By choosing a smart $P(\vec r)$ that is large in the regions of interest, 
%for example where $g(\ts r)$ is large or changing rapidly,  
more samples will be drawn from the important regions, often leading to a more rapid convergence.  
%}}}2

\subsection{Markov Chain Monte Carlo}%{{{2

Markov Chain Monte Carlo (MCMC) provides us with a method for drawing the correct samples from $P$. 
Instead of drawing uncorrelated random samples we use a markow chain to generate a sequence $\{\vec r_n\}$ of vectors.
A Markov chain is caracterized by a transition kernel $q(\vec x|\vec y)$ which is a probabitity density distribution 
of the variable $\vec y$, and which tells us the probability of a transition from $\vec x$ to $\vec y$.
For each $\vec r_n$, we draw a new $\vec r_{n+1}$ from the distribution $q(\vec r_n,\vec y)$. The distribution 
of the vectors in the set 
$\{\vec r_n\}$ will converge towards the distribution $P$ if the following 
conditions are fulfilled (reference):
\begin{itemize}
\item
	$P$ is an invariant of the kernel $q(\vec x|\vec y)$ in the sense that 
	\begin{align}
	\int_{\mathcal D} d\vec r_A\, P(\vec r_A) q(\vec r_A | \vec r_B) = P(\vec r_B) \label{eq221},
	\end{align}
\item
	The chain is ergodetic, which means that any point $\vec r_n$ is axessible in a finite number transitions from any starting point $\vec r_0$. 
	\begin{align}
		0>&q(r_0|r_1)q(r_1|r_2)\dots q(r_{n-1}|r_n),\notag\\
		&\{r_0,r_1,\dots,r_n\}\in\mathcal D 
	\end{align}
	for all $r_0,r_n$. 	
	(more?reference!)
%all $\vec r$ in $\mathcal D$% must be axessible to a random walker
%starting in any point $\vec r_0$ in $\mathcal D$ during a finite number of transitions. 
\end{itemize}

%More sophisticated methods exist, like the Metropolis-Hastings algorithm, where the drawing process is biased.
%To compensate for the biased 
%This method has the drawback that the samples are correlated which makes the analysis of the results somewhat more demanding. But the method also has
%some important advantages. One of them, as we will see later, is that the probabilityfunction $P$ does not need to be normalised. 
%This property comes very handy when dealing with many-particle wavefunctions in quantum mechanics.
%
%$P$ is sampled by generating a random new $\vec r_i$. The new position is then accepted by a probability 
%
%Both statements are actually quite intuitive, and i will now explain why(rewrite sentence):
It is costumary to make the analogy to a set of $\lambda$ biased random walkers, where
the density of walkers $P_w$ is (close to) stationary and proportional to $P$.
This analogy is reasonable as long as the sequence $\{\vec r_n\}$ can be split into $\lambda$ 
subsequences $\{\vec r_n^\lambda\}$ which are uncorrelated. Then one can equally well interpret the samples as $N$ samples
from one markov chain, as $N/\lambda$ samples from $\lambda$ markov chains.

For $P_w$ to be stationary (), the population of walkers in any point $\vec r_A$ must be () constant,
\begin{align}
	\sum_n P_w(\vec r_A) q(\vec r_A|\vec r_n)  \approx	\sum_n P_w(\vec r_n) q(\vec r_n|\vec r_A) \label{eq222}
\end{align} 
This equation resembles('') (\ref{eq221}) since both equaions states that the density is stationary.
%Of course, in a computer simulation the number of walkers is finite, and this equation can only be seen as an approximation.
%The markov chain must also be ergodetic. 
%One are free to choose any equation for the trajectories of the random walkers (any transition kernel) as long as (\ref{eq222})
The condition of detailed balance
\begin{align}
    P_w(\vec r_A) q(\vec r_A|\vec r_B)  =  P_w(\vec r_B) q(\vec r_B|\vec r_A).\label{detbal}
\end{align}
assures that (\ref{eq222}) is fulfilled. 
%This equation is the basis for finding the transition kernels that is used in the metropolis-Hastings algorithm.
As long as the markow chain is ergodetic the walkers will have axcess to all regions of the integration domain.
% ad the entire integration domain will be sampled.
%If not (ergodetic in regions: can be tackled ? )
%}}}2

\subsection{Metropolis-Hasting algorithm}%{{{2
We will now outline a general algorithm for the movement of the random walkers that fullfills (\ref{detbal}).
We introduce new scalar field to our equation $\gamma(\vec r_A,\vec r_B)$, which is a measure of the 
probability of accepting a move from $\vec r_A$ to $\vec r_B$.
We split our transition kernel in two parts
\begin{align}
	q(\vec r_A|\vec r_b) 
		\to 
	\gamma(\vec r_A|\vec r_b)
	g(\vec r_A|\vec r_b).
\end{align} 
i will refer to $g(r_A|r_B)$ as the suggestion density, and $\gamma(\vec r_A|\gamma \vec r_B)$ as the acceptance probability. We set
\begin{align}
&\gamma(\vec r_B|\vec r_A)=1 \text{ when }\notag\\
%\begin{align}
    &P_w(\vec r_A) g(\vec r_A|\vec r_B)  >  P_w(\vec r_B) g(\vec r_B|\vec r_A).
\end{align}
Then, by inserting this equations into (\ref{detbal}), we get
\begin{align} 
	\gamma(\vec r_A|\vec r_B)=max\left(
	\frac
	{P_w(\vec r_B) g(\vec r_B|\vec r_A)}
    {P_w(\vec r_A) g(\vec r_A|\vec r_B)}\, 
	,\, 1 \right) \label{gamma}
\end{align}
By making this choice for $\gamma$, (\ref{detbal}) will be fulfilled and the sequence of $\{r_i\}$ will sample 
$P$. 
%This rule preserves the relative provability of the transitions between $r_A$ and $r_B$, and preserves the 
%density of walkers in each point.  
Note that only the relative ratio between the left and the right hand side of (eq) are 
of interest to us. Therefore, $P$ does not have to be normalized. 
These equations are implemented in the well known Metropolis-Hastings algorithm as follows:
%
%\begin{algorithm}[h]
\begin{algorithmic}
\LOOP{}
\STATE{$\vec r_B\gets$ random from distribution $g(\vec r_T|\vec r_B)$.}
\STATE{$\mathcal X\gets$ random uniform. ($\mathcal X\in[0,1]$)}%??(0,1)}
\STATE{Calculate $\gamma (\vec r_n|\vec r_T))$.}
\IF{$\mathcal X \le \gamma$}
\STATE{$\vec r_{n+1}\gets \vec r_T$}
\ELSE
\STATE{$\vec r_{n+1}\gets \vec r_n$}
\ENDIF{}
\STATE{$n\gets n+1$}
\ENDLOOP{}
\end{algorithmic}
%\label{alg1}
%\caption{\it The Metropolis-Hastings algorithm. $g(\vec r_T|\vec r_B)$ is the Markov transition kernel and 
%$\gamma (\vec r_n|\vec r_T))$ is the acceptance probability as explained in the text.}
%\end{algorithm}
%
%This algorithm is a 
%Write some more about the algo..
%Alternative algos, and their efficiency. refs.
%(more about the algo: convergence and thermalization (implementation issues))

%}}}2

\subsection{Choice of transition kernel} %{{{2

The transition kernel can be written as
\begin{align}
	q(\vec r_n|\vec r_{n+1}) = \gamma(\vec r_n|\vec r_{n+1}) g(\vec r_n|\vec r_{n+1}), 
\end{align}
where $\gamma(\vec r_{n}|\vec r_{n+1})$ is given by eq. (\ref{gamma}). 
The set $\{r_n\}$ will converge tovards the same distribution $P_w$ regardless of which $g(\vec r_n|\vec r_{n+1})$ we use. 
(As long as the markov chain defined by $g(\vec r_n|\vec r_{n+1})$ and $\gamma(\vec r_n|\vec r_{n+1})$ is ergodetic.)
However, the rate of convergence can .. from one sugg.dens. to an other.
%But the rate of convergence can differ widely.

The transition kernel suggested by Markov ... in his original article ... corresponds to a suggestion density $g(\vec r_n|\vec r_{n+1})$ 
that is symmetric around $\vec r_n$ along all axis. In this case $\gamma(\vec r_n|\vec r_{n+1})$ can be expressed as
\begin{align} 
	\gamma(\vec r_A|\vec r_B)=max\left(
	\frac
	{P_w(\vec r_B)}
    {P_w(\vec r_A)}\, 
	,\, 1 \right) \label{gamma_bruteforce}
\end{align}
A common choice is to set
\begin{align}
	g(\vec r_n|\vec r_{n+1}) = \sum_i l\left(\mathcal U(\vec r_{n+1}-\vec r_n) - \frac12\right)
\end{align}
where $l$ is a constant which we will refer to as the steplength, and $\mathcal U(\vec r)$ is the uniform distribution
\begin{equation}
	\mathcal U(\vec r) = \left\{ 
	\begin{split} 
		&1, \text{ for all } \vec r \cdot \vec e_i \in [0,1]\\
		&0, \text{ otherwise }\\
	\end{split}
	\right..
\end{equation} 
Here $\vec e_i$ is the unit vectors of $\mathcal D$. 
By choosing testing different steplengths $l$ will alter the number of 
accepted steps during a simulation, and will have an effect on the rate of convergence. %For example, if nearly all steps are accepted, 
The class of sampling algorithms using a symmetric suggestion density is often referred to as brute force sampling.

An other class of sampling algorithms is based on the ..., where the random walkers ... the Langevin equation for 
The trajectories of ... is gouverned by Langevin dynamics. 
...
...
discretization of the langevin equation.
...
...
Langevin eq and diffusion: not exact for dt>0, but gamma ensures that the sampling is correct. in the special case that 
dt = 0, gamma = 1 always, and every suggested move should be accepted. 
%}}}

%}}}1

\section{Statistical analysis of the data}%{{{1

The statistical error.

%In our simulations, we will sample each point on the track of one single walker. 
%This is possible as long as the system is ergodetic. 
%That is, all $\vec r_n$ in $\mathcal D$ must be axessible to a 
%random walker starting in any point $\vec r_0$ in $\mathcal D$. 
%Otherwise, some parts of the integration domain would not be sampled.
The correlation between ,,
After a certain number of moves, the statistical correlation between
the starting point $\vec r_0$ and the current position $\vec r_n$ will be very weak (there will be no). 
((reference, weaker statement))
If we denote the correlation time $\tau$ and $N$ is the total number of MCMC cycles, 
we will have $\tau/N$ uncorrelates samples. 

When the curve flattens, this does not mean that the configuration of the particles are in step $n$ is uncorrelated with the initial configuration.
Rather it means that the energies is weakly correlated in after n steps when the system is equilibriated.( and that the error is close to its actual value). 
Even if the correlationlength is rather short, say 2000 steps, the thermalization phase might be longer...
Ex: for brute force sampling, the configuration of the particles will never be truly uncorrelated because every move is limited by a finite steplength.
Eg. if the steplength is one, the fastest one particle can move from x=0 to x=1e6 is in 1e6 steps...

A method to find the correlation time.
%}}}1

\section{Quantum monte Carlo}%{{{1 

\subsection{Introduction}%{{{2

%In the Heisenberg image, t
The expectation values for any observable $\hat O(t)$ can be expressed on the general form
\begin{align}
	\expec{ O(t) } = \int d\vec r \Psi(\vec r) \hat O(t) \Psi(\vec r)^*,\label{exv}
\end{align}
where $\Psi(\vec r) = \sum_i \psi_i(\vec r)$, for some basis $\{ \vec \psi_i \}$. 
The integral can also be written 
\begin{align}
	\expec{ O(t) } = \int |\Psi(\vec r)|^2 \frac{ \hat O(t) \Psi(\vec r)^\dagger } {\Psi(\vec r)^\dagger},
\end{align}
Since $|\Psi(\vec r)|^2$ is a probability density distribution we see that
\begin{align}
	\expec{ O(t) } \approx \sum_{i=1}^N \frac{ \hat O(t) \Psi(\vec r_i)^\dagger } {\Psi(\vec r_i)^\dagger}, \label{RFQMC1} 
\end{align}
%
where the set of numbers $\{\vec r_i\}$ are drawn from $|\Psi|^2$.
This sum can be sampled using the Metropolis-Hastings algorithm with the acceptance probability 
\begin{align}
	\gamma(r_i|r_i+1)=\frac
		{g(r_i|r_{i+1})\Psi(r_i)|^2}
		{g(r_{i+1}|r_i)|\Psi(r_{i+1})|^2}.
\end{align} 
$g(r_i|r_{i+1})$ is the suggestion density as desctibed earlier.
%
Note that $\Psi$ do not need to ne normalized in the above equations. This saves us a great deal of work since the normalization 
of most wavefunctions would have to be done numerically for every new trial function.% ????

%}}}2

\subsection{Variational Monte Carlo}%{{{2

Variational Monte Carlo (VMC) is used to calculate the lowest energy state $\epsilon_0$ in a quantum mechanical system.
According to the ritz principle, the expectationvalue for the energy 
\begin{align}
	\epsilon_0\ge\bra{\Psi_T} \hat H \ket{\Psi_T}\,\forall\,\ket{\Psi_T}\in\mathcal H.
\end{align} 
%And if $\bra\Psi\hat H \ket\Psi = \epsilon_0$ then $\ket\Psi=\ket{\phi_0}$ where $\ket{\phi_0}$ is the ground state of the hamiltonian. 
%
This means that any expectation value $\expec{\hat H}$ sets an upper limit to the energy of the system. 
%
This principle allows us to perform a systematic search for the lowest $\expec{\hat H}$ using a set of trial wave function 
$\ket{\Psi_T,\alpha,\beta,\dots}$ where $\alpha,\beta,\dots$ are variational parameters. % that changes the shape of the wave functions.

The search in the parameterspace $(\alpha,\beta, \dots)$ can be done by performing MCMC calculations on a grid. 
More sophisticated methods are also in use, like the conjugate gradient approach which will be described later in this text.

Since the variance $\expec{\hat H^2}-\expec{\hat H}^2=0$ for the eigenfunctions of $\hat H$, the variance is a measure of how close 
$\ket{\Psi_T,\alpha,\beta,\dots}$ is to the 'true' ground state wave function of the system. Even is the lowest energy obtained in our simulations will be closest 
to the ground state energy, the trial function with the lowest variance migt be a better fit to the 'true' groundstate wave function.
%}}}2

\subsection{The trial wave functions}%{{{2

Our system is a closed shell model with the hamiltonian 
\begin{align} 
	\hat H = -\frac{\nabla ^2}2 + \omega |\ts R|^2 - \sum_{i<j}\frac1{r_{ij}}.\label{hamiltonian}
\end{align}
Here $\ts R=(\ts r_1,\ts r_2, \ts r_2, \dots, \ts r_N)$ is a vector containing the position of all electrons in the system.
, and $r_{ij}=|\ts r_i-\ts r_j|$ is the distance between electron $i$ and electron $j$.
The equations
\begin{align}
	\phi_{n_xn_y}^\alpha(\ts r_i) = H_{n_y}(\sqrt{\omega\alpha}y)H_{n_x}(\sqrt{\omega\alpha}x)\notag\\
	\times \exp(-\frac12\omega\alpha|\ts r_i|^2) \label{1pts}
\end{align}
are the eigenstates for the hamiltonian eq. (\ref{hamiltonian}) when $\alpha=1$. $H_n(x)$ is the hermitepolynomials and $\alpha$ is a variational 
parameter $\alpha\in[0,1)$ which represents the ''shielding'' effect that occurs when more than one electron is present in the system. Since the 
electrons carry a negative charge, the efficient external potential seen by the individual particles will be lower than the actual external potential.
%The energy level $n=n_x+n_y$ will be $n+1$ times degenerated. ...

Our trial wave function can be written on the form
\begin{align}
	\Psi(\ts R) = \mathcal D_\downarrow^{\alpha}(\ts R) \mathcal D_\uparrow^\alpha(\ts R) \mathcal J^\beta(\ts R) 
\end{align}
Here $\mathcal D_\uparrow(\ts R)$is a slater determinant consisting of the one particle solutions eq. (\ref{1pts}).
The slater determinant automatically fulfills the Pauli principle stating that any wave function consisting of identical fermions must be 
antisymmetric w.r.t. the exchange of two particles.
$\mathcal D_\downarrow^\alpha(\ts R)$ is the corresponding determinant for the spin down states. Note that in the case of a spin dependent hamiltonian,
$\mathcal D^\alpha_\downarrow \mathcal D^\alpha_\uparrow$ would have to be written as one single slater determinant to preserve the antisymmetry of the wavefunction.
$\mathcal J^\beta(\ts R)$ is the Jastrow factor which removes the singularities in $\hat H \Psi(\ts R)$ at $\ts r_i = \ts r_j$ (ref).
We use a Jastrow factor with one variational parameter $\beta$,
\begin{align}
	&\mathcal J^\beta(\ts R) = \prod_{i<j} \mathcal J_{ij}\notag\\
	&\mathcal J_{ij} = \exp\left( \frac {a_{ij}r_{ij}}{1+\beta r_{ij}} \right)
\end{align}
$a_{ij}=1$ if the spins of particle $i$ and $j$ are parallel and $a_{ij}=\frac13$ if the spins are perpendicular.
See ref. (LE) for a detailed derivation of the Jastrow factor.

The wave functions $\psi^\alpha{n_x,n_y}(\ts R)$ is $n+1$ times degenerated in the $n$'th energy level (starting at $0$). 
In the $s$'th full shell we then have $2\sum_{a=1}^s a$ particles. The basis states used in the different energy levels are listed in table (\ref{tab0}).

According to an article written by Tauts (ref tauts), the energy of the $2$ particle $\omega=1$ quantum dot should be exactly $3$ Hartrees.
The kinetic energy ($\hat H\to -\nabla^2/2$) of the slater matrices can be shown to be (ref,appendix): $2\omega$ Hartrees fo two particles, $10\omega$ 
Hartrees for 6 particles, $28\omega$ Hartrees for 12 particles and $60$ Hartrees for 20 particles. (formula)?. We have used this values to verify parts of the code. 

\begin{table}
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Shell& Electrons & One particle orbitals \\
\hline
1&2& $\psi_{00}$ \\
%\hline
2&6& $\psi_{10},\,\psi_{01}$\\
%\hline
3&12& $\psi_{20},\,\psi_{11},\,\psi_{02}$\\
%\hline
4&20& $\psi_{30},\,\psi_{21},\,\psi_{12},\,\psi_{03}$\\
%\hline
5&30& $\psi_{40},\,\psi_{31},\,\psi_{22},\,\psi_{13},\,\psi_{04}$\\
6&42& $\psi_{50},\,\psi_{41},\,\psi_{32},\,\psi_{23},\,\psi_{14},\,\psi_{05}$\\
\hline
\end{tabular}
\caption{\it 
	This table shows the one particle orbitals that is part of the slater determinants. Only the first orbital state $\psi_{00}$ 
	is needed to fill the first shell. This corresponds to a two electron quantum dot, one spin up and one spin down electron. The
	three first orbitals (six electrons) $\psi_{20},\,\psi_{11},\,\psi_{02}$ are needed to fill the second shell. Six orbitals (twelwe electrons) are needed to fill the third shell and so on.
	%For a quantum dot in the first shell, only $\psi_{00}$ is used, while in the $6$'th shell, all of the orbitals listed above are part of the slater determinants.
}
\label{tab0}
\end{center}
\end{table}



%}}}2

\subsection{Brute force sampling}%{{{2

%}}}2

\subsection{Sampling based on Langevin dynamics}%{{{2
The quantum force

%}}}2

%}}}1

\section{Implementation}%{{{1

The simulator can be efficiently optimized by implementing some mathematical shortcuts. The optimalizations can only be done if we only change the position of one electron at the time.
The metropolis-hastings test is performed each cycle, while the energy and other observables are updated when we have cycled through all electrons. It would also be possible to
evaluate the energy every time a electron has been moved, but this would not be favorable for a system with many electrons since the correlation between the samples would be very high, 
and the evaluation of the energy consumes too many flops. 

\subsection{Optimalization of $\mathcal D^\alpha(\ts R)$}

The inverse $D^{-1}$ of a matric $D$ can be written as the cofactor matrix $C/|D|$. $C$ has the elements $C_{ij}=|S_{ij}|$ where $S_{ij}$ is a sub matrix of $D$ with the $i$'th row and the 
$j$'th coloumn removed. The determinant $|D|$ can be expressed as 
\begin{align}
	&1=\sum_j D_{ij}D^{-1}_{ji}=\sum_jD_{ij}\frac{C_{ji}}{|D|}\notag\\
	&\Rightarrow \sum_jD_{ij}C_{ji}=|D|
\end{align}
From the definition of $C_{ij}$, we see that the $i$'th coloumn of $D^{-1}=C/|D|$ is unchanged if we change the $i$'th row of $D$.
Therefore,
\begin{align}
	&\text{if } D'_{ij}=D_{ij} \forall i\neq k\notag\\
	&\sum_k D'_{kj} D'^{-1}_{jk} = \sum_k D'_{kj} D^{-1}_{jk} = \sum_k D'_{kj} \frac{C^{-1}_{jk}}{|D|}\notag\\ 
	&\Rightarrow \sum_k D'_{kj} D'^{-1}_{jk} = \frac{|D'|}{|D|}\label{eqDet}
\end{align}
This operation scales much better than alternative ways of evaluation the determinant. For example ... 
The problem is that we need to obtain an expression for the inverse matrix $D^{-1}$. By using ... (ref) formula for updating the inverse
matrix, we only need to evaluate the inverse matrix once. Updating the matrix scales as ...

Eq. (\ref{eqDet}) can be used to calculate the ratios
\begin{align}
	&\frac{\mathcal D^\alpha(\ts R')}{D^\alpha(\ts R)}=\sum_{ij} (\mathcal D_{ij})\mathcal D^{-1}_{ji}\\
	&\frac{\nabla^2\mathcal D^\alpha(\ts R)}{D^\alpha(\ts R)}=\sum_{ij} (\partial^2_{r_i}\mathcal D_{ij})\mathcal D^{-1}_{ji}\\
	&\frac{\nabla_i\mathcal D^\alpha(\ts R)}{D^\alpha(\ts R)}=\sum_{j} (\partial_{r_i}\mathcal D_{ij})\mathcal D^{-1}_{ji}
\end{align}
since all expressions can be written as a sum of slater determinants where only one row is changed. 
$\ts R'$ is equal to $\ts R$ except for the coordinates of one electron.
The expressions for $\partial_{r_i}\mathcal D_{ij}$ and
$\partial^2_{r_i}\mathcal D_{ij}$ is calculated by evaluating the analytical expressions 
\begin{align}
	\partial^2_{r_i}\mathcal D_{ij} = \partial^2_{r_i}\psi_i(\ts r_j)=\notag\\
 	\partial_{r_i}\mathcal D_{ij} = \partial_{r_i}\psi_i(\ts r_j)=\notag\\
\end{align}
The expressions is derived using the same strategy as we have used when $(\partial_\alpha)\psi_i(\ts r_j)$ is derived in appendix A.

When only one particle is updated, only one of the determinants $\mathcal D_\downarrow$, $\mathcal D_\uparrow$ needs to be evaluated since ...


\subsection{Optimalization of $\mathcal J^\beta(\ts R)$ and $r_{ij}$}
There are only $n(n-1)/2$ distinct values of $r_{ij}$. Therefore, all values can be stored in a lower tridiaginal matric with zeros on the diagonal.
\begin{align}
	P = 
\left(
\begin{matrix}
	0 		& 0	 	&0 		&\dots	&\\
	r_{21}  & 0 	&0 		& &\\
	r_{31} 	& r_{32}&0 		& &\\
	r_{41} 	& r_{42}&r_{43} & &\\
	\vdots 	& 		& 		& &\\
\end{matrix}
\right)
\end{align}
If we move one electron, only $(n-1)$ elements has to be updated. 

The electron-electron potential can be directly calculated by summing the $(n-1)$ inverse elements of $P$. The Jastrow ratio can be calculated
\begin{align}
	\frac{\mathcal J^\beta(\ts R')}{\mathcal J^\beta(\ts R)}
	= \prod_{i=k\text{ or }j=k} \frac{ \mathcal J_{ij}^\beta(\ts R') }{ \mathcal J_{ij}^\beta(\ts R)}
\end{align} 
when only the electron with coordinates $\ts r_k$ has been moved. This expression can be efficiently evaluated as the exponential of the sum of $2(n-1)$ elements
\begin{align}
	&\frac{\mathcal J^\beta(\ts R')}{\mathcal J^\beta(\ts R)}=
	\exp\left(\sum_{i=k\text{ or }j=k}  j(r_{ij})  - j(r'_{ij}) \right)\notag\\
	&j(r_{ij})=\frac {a_{ij}r_{ij}}{1+\beta r_{ij}}.
\end{align}
The laplacian and the gradient $..$ can be evaluated ...
the gradient: subltract old value, add new.
 
\subsection{Minimization of the wave functions, optimalization of $(\partial_\alpha,\partial_\beta)\expec{E_L}$}
The minimization algorithm we use is the Broyden-Fletcher-Goldfarb-Shannon (BFGS) method. The algorithm itself is copied from (ref numerical recipes), and is a 
so called Quasi-Newton method which iteratively updates the inverse hessian matric $A^{-1}$, which has the property.
\begin{align}
	(\alpha_{min},\beta_{min})= (\alpha_{0},\beta_{0}) - A^{-1}\cdot \nabla \Psi(\ts R,\alpha,\beta)\big|_{(\alpha_{0},\beta_{0})}
\end{align}
The function recieves pointers to a sampling method which returns the energy $\expec {\hat E_L}$ and the gradient of the energy in the point $(\alpha,\beta)$, and updates the
inverse hessian every time new values are returned. The function also recieves a variable \verb gtol  which sets the limit of convergence for the algorithm. If \verb gtol  $=0$ 
the function will continiue iterating till it has localized the point where the gradient is zero.

%starting vals alpha,beta .5,.5
%
The algorithm performs badly if {\verb gtol } is to high or if the number of cycles used to find the energy is to low. On the other hand, minimizing the function
with a high accuracy can be very time consuming. I found that a good strategy is to use %a large {\verb gtol } in the range $\mathcal O(10^{-4})-\mathcal O(10^{-5})$ with
a lesser number of sampling cycles $\mathcal O (10^4)-\mathcal O(10^5)$ to get an estimate to the optimal values for $\alpha,\,\beta$. Then I use a %small {\verb gtol }
%$\mathcal O(10^{-8})$ and a 
larger number of sampling cycles $\mathcal O (10^6)-\mathcal O (10^7)$ to find a better approximation. 
To avoid (unphysical) negative values for $\alpha$ and $\beta$, 
the sampling function are set to return high values for the energy %and a negative gradient 
if the variational parameters gets negative.

The gradient are calculated using analytical expressions for the energy. 
In the cases where comparisons where performed, the analytical expressions gave better results then the numerical derivatives.

COMPARISONS:
Example: %(d/da = (f(a+h)-f(a+h))/2h),h=0.001. For example, for $\omega = 0.28,\, \Delta t=0.05$ with $6$ electrons, I got the results $\alpha=0.87269,\beta=0.32655$ giving an
%energy $E=7.6214(1)$ and a variance ... . $(?).6218(1)$ .

The gradient of the expectationvalue of the energy can be written
\begin{align}
	&\left({\partial_\alpha},{\partial_\beta}\right) \expec{E_L} = \notag\\
		&2\Big\langle\frac{\left({\partial_\alpha},{\partial_\beta}\right)\Psi}{\Psi} E_L\Big\rangle
		-2\Big\langle\frac{\left({\partial_\alpha},{\partial_\beta}\right)\Psi}{\Psi}\Big\rangle\expec{E_L}  
\end{align}
To find the gradients I used the expression
\begin{align}
	&\frac{\left({\partial_\alpha},{\partial_\beta}\right) \Psi(\ts R,\alpha,\beta)}{\Psi(\ts R,\alpha,\beta)}\notag\\
	&= 	\left(\frac{\partial_\alpha \mathcal D(\ts R,\alpha)}{\mathcal D(\ts R,\alpha)}
	, 	\frac{{\partial_\beta} \mathcal J(\ts R,\beta)}{\mathcal J(\ts R,\beta)}\right)
\end{align}
From the definition of the Jastrow eq. (..) we can derive
\begin{align}
	\frac{\partial_\beta \mathcal J(\ts R,\beta)}{ \mathcal J(\ts R,\beta)}=\sum_{k<l}-a
\left( 
	\frac {r_{kl}} {1+\beta r_{kl}}.
\right)^2
\end{align}
The expression is calculated as a sum over the relevant distances $r_{kl}=|\ts r_k-\ts t_l|$ which is already stored as a class variable in the class \verb ipdist  .
The gradient of alpha is calculated using the expression:
\begin{align}
	&\frac{\partial_\alpha \mathcal D(\ts R,\alpha)}{\mathcal D(\ts R,\alpha)}=\sum_i\sum_j(\partial_\alpha \psi_i(\ts r_j,\alpha)) \mathcal D^{-1}_{ji} %INDEXES OK??
\end{align}
\begin{align}
	&\partial_\alpha \psi_i(\ts r_j,\alpha)=\notag\\
	&\psi_{n_{xi},n_{yi}}((x_j,y_j),\alpha)
	\bigg(
	 	n_{xi} \sqrt{\frac\omega\alpha} \frac{H_{n_{xi}-1}(\sqrt{\omega\alpha}x_j)}{H_{n_{xi}}(\sqrt{\omega\alpha}x_j)}\notag\\
	 	&+n_{yi} \sqrt{\frac\omega\alpha} \frac{H_{n_{yi}-1}(\sqrt{\omega\alpha}y_j)}{H_{n_{yi}}(\sqrt{\omega\alpha}y_j)}
		-\frac{\omega}2(x_j^2+y_j^2)
	\bigg)
\end{align}
Here $r_j\to(x_j,y_j)$ and $i\to n_{xi},n_{yi}$.  
The values 	$\partial_\alpha \psi_i(\ts r_j,\alpha)$ are already stored in the matrix $\mathcal D$, and does not have to be calculated. 
All of the analytical expressions above is derivated in appendix A.
%}}}1

\section{Results}

The results are summarized in tables \ref{tab1} and \ref{tab2}. I have used the values $\omega=0.28,0.5,1.0$ and $\Delta t = 0.01,0.25,0.05$ to be able to compare my 
results with ref(...). I have also calculated the values using the same number of cycles. The results are well consistent with the results of (refLE),..

%but the minima obtained seems to give lower energies in most cases cases. 
%This can probably be explained by the fact that I have used a analytical expression to calculate $(\partial_\alpha,\partial_\beta) \expec{E_L}$ while
%() has evaluated the gradient numerically. (List his results in table...)   

The timestep error: We have calculated all energy with three different values for $\Delta t$. 
The timestep error seems to be of the same order as the statistical error for our values of $\Delta t$. 
There is also a general trend in the variance and the statistical error as a function of $\Delta t$. The variance and the statistical error increases as $\Delta t$ gets smaller.
The correlation length((plots?)) are generally increasing when $\Delta t$ gets smaller, probably explaining the increase in the variance and statistical error since the system will use a longer
time to equilibriate after random fluctiations away from equilibrium (eg. if a sequence of mc-steps with a very low metropolis probability are accepted).
%
%The variance is systematically larger for low values of omega. This might indicate that our trial function is further away from the 'true' groundstate solution of the hamiltonian.
%For low $\omega$, the 





 can see that the timestep error   a 


\begin{table}[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$N_{p}$ & $\omega$ & $N_{s}$ & $\alpha_{min}$ & $\beta_{min}$ \\ 
\hline
\hline
 $2$  & $0.28$ & $10^7$ & $0.971$ & $0.252$ \\ 
 $2$  & $0.5$  & $10^7$ & $0.981$ & $0.309$ \\ 
 $2$  & $1$    & $10^7$ & $0.988$ & $0.399$ \\ 	 
\hline
 $6$  & $0.28$ & $10^7$ & $0.873$ & $0.326$ \\ 
 $6$  & $0.5$  & $10^7$ & $0.900$ & $0.413$ \\ 
 $6$  & $1.0$  & $10^7$ & $0.924$ & $0.557$ \\ 
\hline
 $12$ & $0.28$ & $10^7$ & $0.809$ & $0.378$ \\ 
 $12$ & $0.5$  & $10^7$ & $0.845$ & $0.482$ \\ 
 $12$ & $1.0$  & $10^7$ & $0.877$ & $0.658$ \\ 
\hline
\end{tabular}
\end{center}
\caption{{\it 
	Minimization results. $N_p$ is the number of particles and $N_s$ is the number of cycles used to obtain the energies and the gradient during the minimization.
	$\alpha_{min}$ and $\beta_{min}$ is the values of the variational parameters at the minima. 
	%The variable \verb gtol  which sets the convergence criteria
	We have used a $\Delta t=0.05$ during the minimization. 
}}
\label{tab1}
%\hspace{50mm}
\end{table}

\begin{table}%[h!]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$N_{p}$ & $\omega$ & $\Delta t$ & $N_{s}$ & $\expec{E_L}$ & $\sigma^2$ \\ 
\hline
\hline
 $2$ & $0.28$ & $0.01$  & $10^8$ & $1.02213(3)$ & $0.00076627$ \\ 
 $2$ & $0.28$ & $0.025$ & $10^8$ & $1.02219(2)$ & $0.00075989$ \\ 
 $2$ & $0.28$ & $0.05$  & $10^8$ & $1.02218(1)$ & $0.00076032$ \\ 
\hline
 $2$ & $0.5$ & $0.01$  & $10^8$ & $1.66022(3)$ & $0.00116982$ \\ 
 $2$ & $0.5$ & $0.025$ & $10^8$ & $1.66024(2)$ & $0.00116586$ \\ 
 $2$ & $0.5$ & $0.05$  & $10^8$ & $1.66025(1)$ & $0.00116698$ \\ 
\hline
 $2$ & $1.0$ & $0.01$  & $10^8$ & $3.00030(3)$ & $0.00183428$ \\ 
 $2$ & $1.0$ & $0.025$ & $10^8$ & $3.00036(2)$ & $0.00182671$ \\ 
 $2$ & $1.0$ & $0.05$  & $10^8$ & $3.00038(1)$ & $0.00182404$ \\ 
\hline
\hline
 $6$ & $0.28$ & $0.01$  & $10^8$ & $7.6213(1)$ & $0.017831$ \\ 
 $6$ & $0.28$ & $0.025$ & $10^8$ & $7.6214(1)$ & $0.017634$ \\ 
 $6$ & $0.28$ & $0.05$  & $10^8$ & $7.6214(1)$ & $0.017383$ \\ 
\hline
 $6$ & $0.5$ & $0.01$   & $10^8$ & $11.8100(2)$ & $0.043103$ \\ 
 $6$ & $0.5$ & $0.025$  & $10^8$ & $11.8108(1)$ & $0.042331$ \\ 
 $6$ & $0.5$ & $0.05$   & $10^8$ & $11.8101(1)$ & $0.041268$ \\ 
\hline
 $6$ & $1.0$ & $0.01$  & $10^8$ & $20.1898(3)$ & $0.123116$ \\ 
 $6$ & $1.0$ & $0.025$ & $10^8$ & $20.1904(2)$ & $0.119393..$ \\ 
 $6$ & $1.0$ & $0.05$  & $10^8$ & $20.1905(1)$ & $0.115563$ \\ 
\hline
\hline
 $12$ & $0.28$ & $0.01$  & $10^8$ & $25.6994(4)$ & $0.0621312$ \\ 
 $12$ & $0.28$ & $0.025$ & $10^8$ & $25.6993(3)$ & $0.0611159$ \\ 
 $12$ & $0.28$ & $0.05$  & $10^8$ & $25.6993(2)$ & $0.0599358$ \\ 
\hline
 $12$ & $0.5$ & $0.01$  & $10^8$ & $39.2356(4)$ & $0.153526$ \\ 
 $12$ & $0.5$ & $0.025$ & $10^8$ & $39.2345(3)$ & $0.149480$ \\ 
 $12$ & $0.5$ & $0.05$  & $10^8$ & $39.2344(2)$ & $0.145622$ \\ 
\hline
 $12$ & $1.0$ & $0.01$  & $10^8$ & $65.7908(5)$ & $0.441007$ \\ 
 $12$ & $1.0$ & $0.025$ & $10^8$ & $65.7904(3)$ & $0.424902$ \\ 
 $12$ & $1.0$ & $0.05$  & $10^8$ & $65.7903(2)$ & $0.409616$ \\ 
\hline
\end{tabular}
\end{center}
\caption{{\it 
	Monte carlo results using the variational parameters in table (\ref{tab1}).
	Minimization results. $N_p$ is the number of particles and $N_s$ is the number of cycles used to obtain the energies and the gradient during the minimization.
	The thermalization is set to $5\times 10^5$. The error is estimated using blocking analysis of the data. 
%The variable \verb gtol  which sets the convergence criteria
}}
\label{tab2}
%\hspace{50mm}
\end{table}

\begin{appendix}
\section{Derivation of the analytical expressions for $(\partial_\alpha,\partial_\beta)\expec{\hat H}$}%{{{1

The derivative of the jastrow factor can be written
\begin{align}
	&\partial_\beta \mathcal J(\ts R,\beta) =\partial_\beta \prod_{i<j} \mathcal J_{ij}(\ts R,\beta)\notag\\ 
	&= \sum_{k<l}\partial_\beta \mathcal J_{kl}(\ts R,\beta) 
		\prod_{
		%\begin{matrix}
			i<j,
			(i,j)\neq(k,l) 
		%\end{matrix}
		}
	\mathcal J_{ij}(\ts R,\beta).
\end{align}
We are interested in the ratio
\begin{align}
	\frac{\partial_\beta\mathcal J(\ts R,\beta)}{\mathcal J(\ts R,\beta)}
	=\frac{\partial_\beta \prod_{i<j} \mathcal J_{ij}(\ts R,\beta) }
	{\prod_{i<j} \mathcal J_{ij}(\ts R,\beta)}
	= \sum_{k<l}\frac{\partial_\beta \mathcal J_{kl}(\ts R,\beta)}{\mathcal J_{kl}(\ts R,\beta)}.
\end{align}
Evaluation of the terms $\delta_\beta \mathcal J_{kl}(\ts R,\beta)$ is straight forvard, and the full expressions becomes
\begin{align}
	\frac{\partial_\beta\mathcal J(\ts R,\beta)}{\mathcal J(\ts R,\beta)}
	=	
	\sum_{k<l}-a_{kl}
	\left( 
		\frac {r_{kl}} {1+\beta r_{kl}}
	\right)^2.
\end{align}

The differention $\partial_\alpha \mathcal D(\ts R,\alpha)$ can be written on the following form: (using partial derivation(???))
\begin{align}
	&\partial_\alpha\mathcal D(\ts R,\alpha)  \notag\\
=&\left|
\begin{matrix}
	& \partial_\alpha\psi_1(\ts r_1,\alpha) & \partial_\alpha\psi_1(\ts r_2,\alpha) & \partial_\alpha\psi_1(\ts r_3,\alpha) &\dots & \\
	& \psi_2(\ts r_1,\alpha) & \psi_2(\ts r_2,\alpha) & \psi_2(\ts r_3,\alpha) & & \\
	& \psi_3(\ts r_1,\alpha) & \psi_3(\ts r_2,\alpha) & \psi_3(\ts r_3,\alpha) & & \\
	& \vdots & & & & % \vdots & \vdots & \vdots & 
\end{matrix}\right|\notag\\
+&\left|
\begin{matrix}
	& \psi_1(\ts r_1,\alpha) & \psi_1(\ts r_2,\alpha) & \psi_1(\ts r_3,\alpha) &\dots & \\
	& \partial_\alpha\psi_2(\ts r_1,\alpha) & \partial_\alpha\psi_2(\ts r_2,\alpha) & \partial_\alpha\psi_2(\ts r_3,\alpha) & & \\
	& \psi_3(\ts r_1,\alpha) & \psi_3(\ts r_2,\alpha) & \psi_3(\ts r_3,\alpha) & & \\
	& \vdots & & & & % \vdots & \vdots & \vdots & 
\end{matrix}
\right|\notag\\
&+\dots \\
&=\mathcal D^{(1)}(\ts r,\alpha)+\mathcal D^{(2)}(\ts r,\alpha)+\dots
\end{align}
Since only one row in the determinant $\mathcal D^{(i)}(\ts R,\alpha)$ is changed, following the same reasoning as in section ?.?, we can write
\begin{align}
&\frac{\mathcal D^{(i)}(\ts R,\alpha)}{\mathcal D(\ts R,\alpha)}=\sum_j(\partial_\alpha \psi_i(\ts r_j,\alpha)) \mathcal D^{-1}_{ji}
\end{align}
giving the expression
\begin{align} 
&\frac{\partial_\alpha \mathcal D(\ts R,\alpha)}{\mathcal D(\ts R,\alpha)}=\sum_i\sum_j(\partial_\alpha \psi_i(\ts r_j,\alpha)) \mathcal D^{-1}_{ji}.
\end{align}
The terms $\partial_\alpha \psi_i(\ts r_j,\alpha)$ are derived below. 
We write $\psi_i(\ts r_j,\alpha)\to\psi_{n_{xi},n_{yi}}(\ts r_j,\alpha)$. where $n_{xi}$ and $n_{yi}$ is quantum numbers equal to or larger than zero. 
%$n_{xi}+n_{yi}\le n_{xj}+n_{yj}$ when $i<j$. 
\begin{align}
	&\psi_{n_{xi},n_{yi}}((x_i,y_i),\alpha)\notag\\
	&=H_{n_{xi}}(\sqrt{\omega\alpha}x_j)H_{n_{xi}}(\sqrt{\omega\alpha}x_j)\psi_{00}((x_i,y_i),\alpha)
\end{align}
$H_n(x)$ is the hermite polynomials.
Differentiation gives
\begin{align}
	&\partial_\alpha\psi_{n_{xi},n_{yi}}((x_i,y_i),\alpha)=\notag\\
	&\bigg(\partial_\alpha H_{n_{xi}}(\sqrt{\omega\alpha}x_j)\bigg)H_{n_{xi}}(\sqrt{\omega\alpha}x_j)\psi_{00}((x_i,y_i),\alpha)\notag\\
	&+H_{n_{xi}}(\sqrt{\omega\alpha}x_j)\bigg(\partial_\alpha H_{n_{xi}}(\sqrt{\omega\alpha}x_j)\bigg)\psi_{00}((x_i,y_i),\alpha)\notag\\
	&+H_{n_{xi}}(\sqrt{\omega\alpha}x_j)H_{n_{xi}}(\sqrt{\omega\alpha}x_j)\bigg(\partial_\alpha \psi_{00}((x_i,y_i),\alpha)\bigg)
	\label{A1}
\end{align}
We make use of the identity $\delta_x H_n(x)=2nH_{n-1}(x)$,
\begin{align}
	\partial_\alpha H_n(\sqrt{\alpha\omega}x)
	&=\frac{\partial H_n(\sqrt{\alpha\omega}x)}{\partial (\sqrt{\alpha\omega}x)} \frac{\partial (\sqrt{\alpha\omega}x)}{\partial\alpha} \notag\\
	&=nx\sqrt{\frac{\omega}{\alpha}} H_{n-1}(\sqrt{\omega\alpha}x)
	\label{A2}
\end{align}
Derivation of $\psi_{00}((x,y),\alpha)$ yields
\begin{align}
	\partial_\alpha \psi_{00}((x,y),\alpha)=
	-\frac\omega2 (x^2+y^2)	
	\psi_{00}((x,y),\alpha).
	\label{A3}
\end{align}
We combine eq. (\ref{A1}), eq. (\ref{A2}) and eq. (\ref{A3})
\begin{align}
	&\partial_\alpha \psi_i(\ts r_j,\alpha)=\notag\\
	&\psi_{n_{xi},n_{yi}}((x_j,y_j),\alpha)
	\bigg(
	 	n_{xi} \sqrt{\frac\omega\alpha} \frac{H_{n_{xi}-1}(\sqrt{\omega\alpha}x_j)}{H_{n_{xi}}(\sqrt{\omega\alpha}x_j)}\notag\\
	 	&+n_{yi} \sqrt{\frac\omega\alpha} \frac{H_{n_{yi}-1}(\sqrt{\omega\alpha}y_j)}{H_{n_{yi}}(\sqrt{\omega\alpha}y_j)}
		-\frac{\omega}2(x_j^2+y_j^2)
	\bigg)
\end{align}

%}}}1

\section{Pseudocode:}%{{{1
%}}}1
\end{appendix}



%\begin{figure}[h]
%\begin{center}
%	\includegraphics[width=8cm]{../datafiles/4000steps6partw1emin.eps}
%\end{center}
%\caption{{\it\small Example of the path of one walker during 4000 steps. The dotted line shows the path and the crosses 
%marks the samplingpoints}}
%\end{figure}
%
%\begin{figure}[h]
%\begin{center}
%	\includegraphics[width=8cm]{../plots/1pd_2ptdt05emin2e6c.eps}
%\end{center}
%\caption{{\it\small Normalysized densityplot for 2 pt. $\alpha=0.98$, $\omega=0.4$, $\delta t=0.05$. $10^6$ cycles. }}
%\end{figure}
%
%\begin{figure}[h]
%\begin{center}
%	\includegraphics[width=8cm]{../plots/1pd6ptw1emin.eps}
%\end{center}
%\caption{{\it\small Normalized density plot for 6 pt. $\alpha=0.92$, $\omega=0.565$, $\delta t=0.05$. $10^6$ cycles. }}
%\end{figure}
%
%\begin{figure}[h]
%begin{center}
%	\includegraphics[width=8cm]{../plots/1pd_12ptdt05emin2e6c.eps}
%\end{center}
%\caption{{\it\small Normalized density plot for 12 pt. $\alpha=0.87$, $\omega=0.68$, $\delta t=0.05$. $10^6$ cycles. }}
%\end{figure}

\end{document}

% vim:foldmethod=marker

